{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage.util import view_as_windows\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from skimage.measure import moments_central, moments_hu, moments_normalized\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './data/test/image/0.png'\n",
    "mask_path = './data/test/mask/0.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    green_photo = img[:,:,1]\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clahe_photo = clahe.apply(green_photo)\n",
    "\n",
    "    return clahe_photo\n",
    "\n",
    "def get_img_and_mask(img_path, mask_path):\n",
    "    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.cvtColor(cv2.imread(str(mask_path)), cv2.COLOR_BGR2GRAY)\n",
    "    mask = np.array(mask).astype(np.uint8)\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "def basic_approach_postprocess(masks):\n",
    "    postprocessed_masks = [cv2.threshold(mask, 255, 255, cv2.THRESH_BINARY)[1] for mask in masks]\n",
    "\n",
    "    postprocessed_masks = np.array(postprocessed_masks).astype(np.uint8)\n",
    "\n",
    "    # makeing black everything that is not connected to the biggest white object\n",
    "    for i in range(len(postprocessed_masks)):\n",
    "        nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(postprocessed_masks[i])\n",
    "        max_label = 0\n",
    "        max_size = 0\n",
    "        for j in range(1, nlabels):\n",
    "            if stats[j, cv2.CC_STAT_AREA] > max_size:\n",
    "                max_label = j\n",
    "                max_size = stats[j, cv2.CC_STAT_AREA]\n",
    "        postprocessed_masks[i][labels != max_label] = 0\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    postprocessed_masks = [cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel) for mask in postprocessed_masks]\n",
    "\n",
    "    return postprocessed_masks\n",
    "\n",
    "def plot(img, mask, pred):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(pred, cmap='gray')\n",
    "    plt.title('Prediction')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def sharr(img_path, mask_path):\n",
    "    img, mask = get_img_and_mask(img_path, mask_path)\n",
    "    prep_img = preprocess(img)\n",
    "\n",
    "    scharr_x = cv2.Scharr(prep_img, cv2.CV_64F, 1, 0)\n",
    "    scharr_y = cv2.Scharr(prep_img, cv2.CV_64F, 0, 1)\n",
    "    scharr = cv2.magnitude(scharr_x, scharr_y)\n",
    "\n",
    "    pred = basic_approach_postprocess([scharr])[0]\n",
    "\n",
    "    plot(img, mask, pred)\n",
    "\n",
    "def features_extracting(photo_batch):\n",
    "    color_variance = np.var(photo_batch, axis=(1, 2))\n",
    "    hu_moments = np.zeros((len(photo_batch), 7))\n",
    "    for i, photo in enumerate(photo_batch):\n",
    "        mu = moments_central(photo, order=5)\n",
    "        nu = moments_normalized(mu)\n",
    "        hu = moments_hu(nu)\n",
    "        hu_moments[i] = hu\n",
    "\n",
    "    features = np.concatenate((color_variance.reshape(-1, 1), hu_moments), axis=1)\n",
    "    return features\n",
    "\n",
    "def create_dataset(photos, masks, size=5, step=1):\n",
    "    ds_photos = []\n",
    "    ds_masks = []\n",
    "    for photo, mask in zip(photos, masks):\n",
    "        photo_pad = np.pad(photo, ((size // 2, size // 2), (size // 2, size // 2)), mode='constant')\n",
    "        mask_pad = np.pad(mask, ((size // 2, size // 2), (size // 2, size // 2)), mode='constant')\n",
    "\n",
    "        # Extract patches\n",
    "        patches_photo = view_as_windows(photo_pad, (size, size), step=step).reshape(-1, size, size)\n",
    "        patches_mask = view_as_windows(mask_pad, (size, size), step=step).reshape(-1, size, size)\n",
    "\n",
    "        # Extract features for all patches in a batch\n",
    "        batched_features = features_extracting(patches_photo)\n",
    "\n",
    "        ds_photos.extend(batched_features)\n",
    "        ds_masks.extend(patches_mask[:, size // 2, size // 2])  # Extract central pixel from each mask patch\n",
    "\n",
    "    return ds_photos, ds_masks\n",
    "\n",
    "def kneighbors(img_path, mask_path):\n",
    "    model_path = \"./models/knnclassifier.pkl\"\n",
    "\n",
    "    img, mask = get_img_and_mask(img_path, mask_path)\n",
    "    img = preprocess(img)\n",
    "\n",
    "    photo_batch, _ = create_dataset([img], [mask], size=5, step=1)\n",
    "\n",
    "    model = pickle.load(open(model_path))\n",
    "    pred = model.predict(photo_batch)\n",
    "\n",
    "    y_pred_img = np.zeros((512, 512))\n",
    "    for i in range(0, 512):\n",
    "        for j in range(0, 512):\n",
    "            y_pred_img[i][j] = pred[i * 512 + j]\n",
    "\n",
    "    plot(img, mask, pred)\n",
    "\n",
    "def unet(img_path, mask_path):\n",
    "    model_path = \"./models/model.pth\"\n",
    "\n",
    "    img, mask = get_img_and_mask(img_path, mask_path)\n",
    "    prep_img = preprocess(img)\n",
    "\n",
    "    model = smp.Unet()\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    test_image_transform = v2.Compose([\n",
    "        v2.ToPILImage(),\n",
    "        v2.Grayscale(num_output_channels=1),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=[0.485], std=[0.229]),\n",
    "        ])\n",
    "\n",
    "    model.eval()\n",
    "    x = test_image_transform(prep_img)\n",
    "    x = x[:, None, :, :]\n",
    "    pred = model(x)\n",
    "\n",
    "    plot(img, mask, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sharr(img_path, mask_path)\n",
    "# kneighbors(img_path, mask_path)\n",
    "unet(img_path, mask_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
